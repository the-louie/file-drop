# Database Schema & Data Management

This document details the database structure and data management policies.

## Database Schema

### Tables

**1. uploaded_files** - Stores completed file metadata

```sql
CREATE TABLE uploaded_files (
  fid INTEGER PRIMARY KEY AUTOINCREMENT,
  fileName TEXT,                      -- Original filename
  sha TEXT UNIQUE,                    -- Short hash for URL
  timestamp INTEGER DEFAULT (strftime('%s', 'now')),
  collectionID TEXT,                  -- UUID for grouping
  fileSize INTEGER,                   -- Bytes
  remote_ip TEXT                      -- Hashed IP (SHA-256)
);

CREATE INDEX idx_uploaded_files_collectionID ON uploaded_files(collectionID);
CREATE INDEX idx_uploaded_files_remote_ip_timestamp ON uploaded_files(remote_ip, timestamp);
```

**2. uploaded_chunks** - Temporary chunk tracking

```sql
CREATE TABLE uploaded_chunks (
  cid INTEGER PRIMARY KEY AUTOINCREMENT,
  uuid TEXT,                          -- Upload session ID
  filename TEXT,                      -- Encrypted chunk filename
  chunk_id INT,                       -- Chunk index
  timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_uploaded_chunks_uuid ON uploaded_chunks(uuid);
```

**3. audit_log** - Security event logging

```sql
CREATE TABLE audit_log (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  timestamp INTEGER DEFAULT (strftime('%s', 'now')),
  event_type TEXT,                    -- LOGIN_SUCCESS, UPLOAD_BLOCKED, etc.
  remote_ip TEXT,                     -- Hashed IP
  username TEXT,
  details TEXT,                       -- JSON blob
  status TEXT                         -- SUCCESS, FAILURE, BLOCKED
);
```

**4. users** - Multi-user authentication

```sql
CREATE TABLE users (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  username TEXT UNIQUE NOT NULL,
  password_hash TEXT NOT NULL,        -- bcrypt hash
  is_admin INTEGER DEFAULT 0,
  created_at INTEGER DEFAULT (strftime('%s', 'now')),
  last_login INTEGER
);

CREATE INDEX idx_users_username ON users(username);
```

---

## Data Management

### Cleanup Jobs (Hourly)

**1. Orphaned Chunks (`cleanupOrphanedChunks`)**
```sql
DELETE FROM uploaded_chunks
WHERE timestamp < datetime('now', '-24 hours');
```
- Chunks older than 24 hours (failed/incomplete uploads)
- Deletes both DB records and encrypted files
- Runs on startup + hourly

**2. File Retention (`cleanupOldFiles`)**
```sql
DELETE FROM uploaded_files
WHERE timestamp < strftime('%s', 'now', '-<retention_days> days');
```
- Configurable via `file_retention_days` (default: 30)
- Set to 0 to keep files forever
- Deletes both DB records and files from disk
- Runs on startup + hourly

**3. Audit Log Retention (`cleanupOldAuditLogs`)**
```sql
DELETE FROM audit_log
WHERE timestamp < strftime('%s', 'now', '-<retention_days> days');
```
- Configurable via `audit_log_retention_days` (default: 90)
- Set to 0 to keep logs forever
- Privacy/GDPR compliance
- Runs on startup + hourly

### Quota Enforcement

**Parallel Query Execution:**

```javascript
const queries = [
  dbGet("SELECT SUM(fileSize) FROM uploaded_files"),
  dbGet("SELECT SUM(fileSize) FROM uploaded_files WHERE remote_ip = ? AND timestamp > ?"),
  dbGet("SELECT COUNT(*) FROM uploaded_files WHERE remote_ip = ? AND timestamp > ?")
];

const [globalRow, ipStorageRow, ipFilesRow] = await Promise.all(queries);

// Check quotas in order:
// 1. Global storage (max_storage_bytes)
// 2. Per-IP daily storage (per_ip_daily_bytes)
// 3. Per-IP daily file count (per_ip_daily_files)
```

**Quota Rejection:**
- HTTP 507 (Insufficient Storage) - Global quota
- HTTP 429 (Too Many Requests) - Per-IP quota
- Retry-After header indicates when to retry
- Client displays user-friendly error message

### Performance Optimizations

**Database Indices:**

Impact: Transforms O(n) full table scans into O(log n) index lookups

```sql
-- Quota queries (used every upload)
idx_uploaded_files_remote_ip_timestamp (remote_ip, timestamp)

-- Collection views
idx_uploaded_files_collectionID (collectionID)

-- Chunk merges
idx_uploaded_chunks_uuid (uuid)

-- User lookups
idx_users_username (username)
```

**Parallel Queries:**

Before: Sequential nested callbacks (4 levels deep, ~300ms)
```javascript
db.get(query1, (err, row1) => {
  db.get(query2, (err, row2) => {
    db.get(query3, (err, row3) => {
      // Finally check quotas
    });
  });
});
```

After: Parallel execution (~100ms, 3x faster)
```javascript
const [row1, row2, row3] = await Promise.all([
  dbGet(query1),
  dbGet(query2),
  dbGet(query3)
]);
// Check quotas
```

---

**Copyright Â© 2025 the_louie**

